{"cells":[{"cell_type":"markdown","source":["# Mt. Everest Data Cleansing \n","Silver Layer Processing"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ea68e5cd-d5cf-49eb-96cd-4d0b5a84dc5d"},{"cell_type":"markdown","source":["### Importing libs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7957a7fa-3c55-428b-9ce6-23e40b0f95f1"},{"cell_type":"code","source":["from pyspark.sql import functions as F, types as T, DataFrame as Fr, Column as C "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:17.8586727Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:24.8004945Z","execution_finish_time":"2025-12-17T09:33:25.2270883Z","parent_msg_id":"465dd02e-7734-4abf-82aa-b6113c2ee542"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7c7d8a5-c2df-49e7-b0e8-2b9b6c26c0e7"},{"cell_type":"markdown","source":["### Himalayan Database data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1bf19fe-5da7-45d9-8590-e83bbe639586"},{"cell_type":"markdown","source":["Starting with the Expedition data, I'm going to create a dataframe for each file to start working with it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b30024aa-e031-4a9c-afa4-08eb2642a072"},{"cell_type":"code","source":["df_expeditions = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/expeditions.csv\")\n","df_peaks = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/peaks.csv\")\n","df_members = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/members.csv\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:17.9559308Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:25.2290154Z","execution_finish_time":"2025-12-17T09:33:40.0108293Z","parent_msg_id":"0cfa5965-e336-4013-872a-d427bff017ae"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"f6a7042c-6679-4ec2-b0c6-c9e7dea601a1"},{"cell_type":"markdown","source":["I just want to import data about Everest, so I'll start with removing all other rows from Peaks - this with be the starting point for the upcoming filters."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81b2ccf5-4d4e-465e-b450-f43f3c8a35e5"},{"cell_type":"code","source":["df_peaks = df_peaks.filter(df_peaks.PKNAME == 'Everest')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.1510519Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:40.0129445Z","execution_finish_time":"2025-12-17T09:33:40.2864237Z","parent_msg_id":"81a9dae7-f0a3-41fe-a5e2-f504cf543905"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"072d0e0a-f358-4848-95ee-7ca215a8ba4a"},{"cell_type":"markdown","source":["Next I'm doing the same concerning the Expeditions, but for this I need to do three steps:\n","1. Rename the PEAKID column in the Expeditions dataframe\n","2. Join the Expeditions with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe \n","\n","I started by renaming the column PEAKID from the Expeditions to PEAKID_EXP so that when dropping the extra columns there would not be a duplicate PEAKID column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f3c4450-8b7b-4343-bbb4-b392994e18c4"},{"cell_type":"code","source":["df_expeditions = df_expeditions.withColumnRenamed('PEAKID', 'PEAKID_EXP')\n","df_expeditions = df_expeditions.join(df_peaks, df_expeditions.PEAKID_EXP == df_peaks.PEAKID)\n","df_expeditions = df_expeditions.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_expeditions.head(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.2666761Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:40.2885957Z","execution_finish_time":"2025-12-17T09:33:41.1089795Z","parent_msg_id":"63cc17c6-8b4e-47b5-95e1-365b28c207c3"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"1bc20969-27aa-43ad-b927-8026a28f9b24"},{"cell_type":"markdown","source":["And now I'll filter the Members dataframe using the Peaks dataframe. So again:\n","1. Rename the PEAKID column in the Members dataframe\n","2. Join the Members with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e52dc4a5-0294-4d30-868f-0211d75ced15"},{"cell_type":"code","source":["df_members = df_members.withColumnRenamed('PEAKID', 'PEAKID_MEM')\n","df_members = df_members.join(df_peaks, df_members.PEAKID_MEM == df_peaks.PEAKID)\n","df_members = df_members.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_members.head(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.3700532Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:41.111287Z","execution_finish_time":"2025-12-17T09:33:41.919842Z","parent_msg_id":"51847968-0aee-4d38-b863-cea7fc5ae63e"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"5cc77184-a962-4f78-b389-a7ca600ccbf6"},{"cell_type":"markdown","source":["After filtering these files, I'm changing the types and names for each column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1d18ad9-0f4b-4e20-b696-b7d6f3eed804"},{"cell_type":"code","source":["df_peaks = (\n","    df_peaks\n","    .select(\n","        F.col('PEAKID').alias('PeakID'),\n","        F.col('PKNAME').alias('PeakName'),\n","        F.col('PKNAME2').alias('AlternativePeakName'),\n","        F.col('LOCATION').alias('Location'),\n","        F.col('HEIGHTM').cast(T.IntegerType()).alias('Height_m'),\n","        F.col('OPEN').alias('Open'),\n","        F.col('PEXPID').alias('ExpeditionID'),\n","        F.date_format(F.to_date(F.concat_ws(' ', df_peaks.PYEAR, df_peaks.PSMTDATE), 'yyyy MMM dd'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.col('PSUMMITERS').alias('Summiters'),\n","        F.col('PCOUNTRY').alias('SummitersCountry')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.4621751Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:41.9217043Z","execution_finish_time":"2025-12-17T09:33:42.1895459Z","parent_msg_id":"07f4ceab-e9e2-48a9-9bfe-830e75b1a3b8"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52477fce-087e-4fab-8a1f-bda1a67f047c"},{"cell_type":"code","source":["df_expeditions = (\n","    df_expeditions\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_EXP').alias('PeakID'),\n","        F.col('YEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('ROUTE1').alias('Route'),\n","        F.col('NATION').alias('CountryOfOrigin'),\n","        F.col('LEADERS').alias('ExpeditionLeaders'),\n","        F.col('SPONSOR').alias('Sponsor'),\n","        F.col('SUCCESS1').alias('SummitSuccess'),\n","        F.date_format(F.to_date(F.col('BCDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('BaseCampDate'),\n","        F.date_format(F.to_date(F.col('SMTDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('SMTTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('SMTTIME'), 3, 2)).alias('SummitTime'),\n","        F.col('SMTDAYS').cast(T.ShortType()).alias('DaysToSummit'),\n","        F.date_format(F.to_date(F.col('TERMDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('ExpeditionEndDate'),\n","        F.col('TERMNOTE').alias('Notes'),\n","        F.col('CAMPS').cast(T.ByteType()).alias('Camps'),\n","        F.col('Rope').alias('RopeUsed_m'),\n","        F.col('TOTMEMBERS').cast(T.ByteType()).alias('TotalMembers'),\n","        F.col('SMTMEMBERS').cast(T.ByteType()).alias('SummitMembers'),\n","        F.col('MDEATHS').cast(T.ByteType()).alias('MembersDeath'),\n","        F.col('TOTHIRED').cast(T.ByteType()).alias('TotalHired'),\n","        F.col('SMTHIRED').cast(T.ByteType()).alias('SummitHired'),\n","        F.col('HDEATHS').cast(T.ByteType()).alias('HiredDeath'),\n","        F.col('O2USED').alias('UsedO2'),\n","        F.col('OTHERSMTS').alias('OtherSummits'),\n","        F.col('CAMPSITES').alias('Campsites'),\n","        F.col('ACCIDENTS').alias('Accidents'),\n","        F.col('ACHIEVMENT').alias('Achievement')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.5583529Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:42.1914078Z","execution_finish_time":"2025-12-17T09:33:43.010983Z","parent_msg_id":"182fe340-e970-494e-9818-a9f8c3f13757"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c052e34e-aa8d-4c66-9172-bcfa2a15234e"},{"cell_type":"code","source":["df_members = (\n","    df_members\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_MEM').alias('PeakID'),\n","        F.col('MYEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('FNAME').alias('FirstName'),\n","        F.col('LNAME').alias('LastName'),\n","        F.col('Sex').alias('Gender'),\n","        F.col('YOB').cast(T.ShortType()).alias('YearOfBirth'),\n","        F.col('CALCAGE').cast(T.ByteType()).alias('Age'),\n","        F.col('CITIZEN').alias('Nationality'),\n","        F.col('STATUS').alias('Status'),\n","        F.col('OCCUPATION').alias('Occupation'),\n","        F.col('LEADER').alias('Leader'),\n","        F.col('SUPPORT').alias('Support'),\n","        F.col('DISABLED').alias('Disabled'),\n","        F.col('SHERPA').alias('Sherpa'),\n","        F.col('TIBETAN').alias('Tibetan'),\n","        F.col('MSUCCESS').alias('SummitSuccess'),\n","        F.col('MSOLO').alias('SoloSummit'),\n","        F.date_format(F.to_date(F.col('MSMTDATE1'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('MSMTTIME1'), 1, 2), F.lit(\":\"), F.substring(F.col('MSMTTIME1'), 3, 2)).alias('SummitTime'),\n","        F.col('MROUTE1').alias('MountainRoute'),\n","        F.col('MASCENT1').alias('AscentRoute'),\n","        F.col('MO2USED').alias('UsedO2'),\n","        F.col('DEATH').alias('Death'),\n","        F.date_format(F.to_date(F.col('DEATHDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('DeathDate'),\n","        F.concat(F.substring(F.col('DEATHTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('DEATHTIME'), 3, 2)).alias('DeathTime'),\n","        F.col('DEATHNOTE').alias('DeathNote'),\n","        F.col('INJURY').alias('Injury'),\n","        F.date_format(F.to_date(F.col('INJURYDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('InjuryDate'),\n","        F.concat(F.substring(F.col('INJURYTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('INJURYTIME'), 3, 2)).alias('InjuryTime')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.6458476Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:43.0133639Z","execution_finish_time":"2025-12-17T09:33:43.7864432Z","parent_msg_id":"95866bda-14b4-4df3-8113-9304d982cbd3"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f917f830-9660-4f29-b23d-daec9b9f9255"},{"cell_type":"markdown","source":["### Weather data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d3890cd-663c-4ee5-86f0-67b83a25e81b"},{"cell_type":"markdown","source":["Next up I'm going to treat the weather data, starting with the current weather data.\n","\n","The values here are stored in a \"key: value\" format, so I need to extract each value in order to create a table with the columns I'm interested in.\n","\n","It's important to note that Datetime is reflecting the timezone in Mount Everest, which is GMT + 8h."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd76d164-d408-4820-8065-40dc113dfbb5"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_weather_data.json\")\n","df_current_weather = (df.select(\n","    F.date_format(F.col('current.time'), 'yyyy-MM-dd').alias('Date'),\n","    F.date_format(F.col('current.time'), 'HH:mm').alias('Time'),\n","    F.date_format(F.col('daily.sunrise').getItem(0), 'HH:mm').alias('Sunrise'), \n","    F.date_format(F.col('daily.sunset').getItem(0), 'HH:mm').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('current.weather_code').alias('WeatherCode'),\n","    F.col('current.precipitation').alias('Precipitation_mm'),\n","    F.col('current.temperature_2m').alias('Temperature_ºC'),\n","    F.col('current.wind_speed_10m').alias('WindSpeed_km/h'),\n","    F.col('current.wind_gusts_10m').alias('WindGusts_km/h'),\n","    F.col('current.relative_humidity_2m').alias('RelativeHumidity_%'),\n","    F.col('current.snowfall').alias('Snowfall_cm')\n","))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.7340052Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:43.7882477Z","execution_finish_time":"2025-12-17T09:33:45.3585844Z","parent_msg_id":"7e97cfb8-d133-47c1-9876-f14cc82aa1f4"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false},"editable":true,"run_control":{"frozen":false}},"id":"df933deb-4a48-40f2-81ff-c8dddf40d8b8"},{"cell_type":"markdown","source":["Next I'm organizing the information for historical weather data. I need to expand the values in different columns because each column had the results stored for all dates and I need to convert that to multiple rows."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"063ca284-332a-4be1-a928-3b66a73cfa62"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_hist_weather_data.json\")\n","df = df.select(\n","    F.col('hourly.time').alias('Datetime'),\n","    F.col('daily.time').alias('Date'),\n","    F.col('daily.sunrise').alias('Sunrise'),\n","    F.col('daily.sunset').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('hourly.weather_code').alias('WeatherCode'),\n","    F.col('hourly.precipitation').alias('Precipitation'), \n","    F.col('hourly.temperature_2m').alias('Temperature'),\n","    F.col('hourly.wind_speed_10m').alias('WindSpeed'),\n","    F.col('hourly.wind_gusts_10m').alias('WindGusts'),\n","    F.col('hourly.relative_humidity_2m').alias('RelativeHumidity'),\n","    F.col('hourly.snowfall').alias('Snowfall')\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.8463994Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:45.3604602Z","execution_finish_time":"2025-12-17T09:33:49.1016209Z","parent_msg_id":"61d6cd5a-9d65-440b-8bb2-3797d4434b11"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false},"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8d3f6414-c2a9-4e2c-afdf-ed8673be9a5c"},{"cell_type":"code","source":["df_hist_weather = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Datetime'),\n","            F.col('WeatherCode'),\n","            F.col('Precipitation'),\n","            F.col('Temperature'),\n","            F.col('WindSpeed'),\n","            F.col('WindGusts'),\n","            F.col('RelativeHumidity'),\n","            F.col('Snowfall')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.date_format(F.col('zipped.Datetime'), 'yyyy-MM-dd').alias('Date'),\n","        F.date_format(F.col('zipped.Datetime'), 'HH:mm').alias('Time'),\n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone'),\n","        F.col('zipped.WeatherCode').alias('WeatherCode'),\n","        F.col('zipped.Precipitation').alias('Precipitation_mm'),\n","        F.col('zipped.Temperature').alias('Temperature_ºC'),\n","        F.col('zipped.WindSpeed').alias('WindSpeed_km/h'),\n","        F.col('zipped.WindGusts').alias('WindGusts_km/h'),\n","        F.col('zipped.RelativeHumidity').alias('RelativeHumidity_%'),\n","        F.col('zipped.Snowfall').alias('Snowfall_cm')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:18.9590169Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:49.1034719Z","execution_finish_time":"2025-12-17T09:33:51.7886343Z","parent_msg_id":"62cda436-4d3d-4228-b280-beed17666d7a"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386758a6-2be8-4964-ac7a-42a51c45c2de"},{"cell_type":"markdown","source":["I'm creating a separate dataframe to store the sunrise and sunset values."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3738379b-71ed-4621-84ac-3c5629357d82"},{"cell_type":"code","source":["df_weather_daily = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Date'),\n","            F.col('Sunrise'),\n","            F.col('Sunset')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.col('zipped.Date').alias('Date'),\n","        F.date_format(F.col('zipped.Sunrise'), 'HH:mm').alias('Sunrise'),   \n","        F.date_format(F.col('zipped.Sunset'), 'HH:mm').alias('Sunset'),     \n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:33:19.0281677Z","session_start_time":null,"execution_start_time":"2025-12-17T09:33:51.7904963Z","execution_finish_time":"2025-12-17T09:33:52.1472504Z","parent_msg_id":"218067e7-fc3e-41da-b683-23fce28cd37f"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"541f6703-0e24-4d39-802f-45c7a6a720c6"},{"cell_type":"markdown","source":["I now need to import the information about the weather codes that is also stored in a csv file and that was previously imported directly to the lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6c625eb-b84e-4dac-9377-aa4e40b7b38b"},{"cell_type":"code","source":["df_weather_codes = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/weather_codes.csv\")\n","df_weather_codes = (df_weather_codes\n","    .select(\n","        F.col('Code').cast(T.ShortType()),\n","        F.col('Description')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:42:42.3645881Z","session_start_time":null,"execution_start_time":"2025-12-17T09:42:42.3657677Z","execution_finish_time":"2025-12-17T09:42:43.2033125Z","parent_msg_id":"be645ff5-38c6-4f01-9fe1-b1b196dc2742"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b4c635d-cd8b-4fdc-98a1-f7b93ddf9cb0"},{"cell_type":"markdown","source":["Finally, I'm saving this information as a table for the gold layer."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"029109dc-7528-4851-a1b9-38afffc5e3b1"},{"cell_type":"code","source":["#df_expeditions.write.mode('overwrite').saveAsTable('expeditions_silver')\n","#df_peaks.write.mode('overwrite').saveAsTable('peaks_silver')\n","#df_members.write.mode('overwrite').saveAsTable('members_silver')\n","#df_current_weather.write.mode('overwrite').saveAsTable('weather_current_silver')\n","#df_hist_weather.write.mode('overwrite').saveAsTable('weather_hist_silver')\n","df_weather_codes.write.mode('overwrite').saveAsTable('weather_codes_silver')\n","#df_weather_daily.write.mode('overwrite').saveAsTable('weather_daily_silver')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"834d25a1-42d7-4c61-9da3-11974cfeb138","normalized_state":"finished","queued_time":"2025-12-17T09:43:01.814814Z","session_start_time":null,"execution_start_time":"2025-12-17T09:43:01.8160912Z","execution_finish_time":"2025-12-17T09:43:08.1024492Z","parent_msg_id":"96225723-8ea0-4145-8c7f-b3ced17ca6ec"},"text/plain":"StatementMeta(, 834d25a1-42d7-4c61-9da3-11974cfeb138, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"c4fe32fc-16e6-40ef-93b5-9c090eaddef1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"e8d581c4-dedd-451f-841b-d7aad8f1a690","known_lakehouses":[{"id":"e8d581c4-dedd-451f-841b-d7aad8f1a690"}],"default_lakehouse_name":"EverestWeather","default_lakehouse_workspace_id":"fc9513c7-33cd-4dd8-9e35-9951846a8312"}}},"nbformat":4,"nbformat_minor":5}