{"cells":[{"cell_type":"markdown","source":["# Mt. Everest Data Cleansing \n","Silver Layer Processing"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ea68e5cd-d5cf-49eb-96cd-4d0b5a84dc5d"},{"cell_type":"markdown","source":["### Importing libs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7957a7fa-3c55-428b-9ce6-23e40b0f95f1"},{"cell_type":"code","source":["from pyspark.sql import functions as F, types as T, DataFrame as Fr, Column as C "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:38.2881301Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:38.2891911Z","execution_finish_time":"2025-12-16T14:43:38.5630342Z","parent_msg_id":"a92a85f5-6ca1-4bae-bccc-88dc3c4d66f7"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 26, Finished, Available, Finished)"},"metadata":{}}],"execution_count":24,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7c7d8a5-c2df-49e7-b0e8-2b9b6c26c0e7"},{"cell_type":"markdown","source":["### Himalayan Database data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1bf19fe-5da7-45d9-8590-e83bbe639586"},{"cell_type":"markdown","source":["Starting with the Expedition data, I'm going to create a dataframe for each file to start working with it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b30024aa-e031-4a9c-afa4-08eb2642a072"},{"cell_type":"code","source":["df_expeditions = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/expeditions.csv\")\n","df_peaks = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/peaks.csv\")\n","df_members = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/members.csv\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:38.3987225Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:38.565065Z","execution_finish_time":"2025-12-16T14:43:40.9627161Z","parent_msg_id":"092355b4-7db5-481b-84e6-db72ebdb3caf"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 27, Finished, Available, Finished)"},"metadata":{}}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"f6a7042c-6679-4ec2-b0c6-c9e7dea601a1"},{"cell_type":"markdown","source":["I just want to import data about Everest, so I'll start with removing all other rows from Peaks - this with be the starting point for the upcoming filters."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81b2ccf5-4d4e-465e-b450-f43f3c8a35e5"},{"cell_type":"code","source":["df_peaks = df_peaks.filter(df_peaks.PKNAME == 'Everest')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:38.8165778Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:40.9646525Z","execution_finish_time":"2025-12-16T14:43:41.2715767Z","parent_msg_id":"d0bcf9a7-ec8d-4eec-b064-e97455f22ce1"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 28, Finished, Available, Finished)"},"metadata":{}}],"execution_count":26,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"072d0e0a-f358-4848-95ee-7ca215a8ba4a"},{"cell_type":"markdown","source":["Next I'm doing the same concerning the Expeditions, but for this I need to do three steps:\n","1. Rename the PEAKID column in the Expeditions dataframe\n","2. Join the Expeditions with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe \n","\n","I started by renaming the column PEAKID from the Expeditions to PEAKID_EXP so that when dropping the extra columns there would not be a duplicate PEAKID column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f3c4450-8b7b-4343-bbb4-b392994e18c4"},{"cell_type":"code","source":["df_expeditions = df_expeditions.withColumnRenamed('PEAKID', 'PEAKID_EXP')\n","df_expeditions = df_expeditions.join(df_peaks, df_expeditions.PEAKID_EXP == df_peaks.PEAKID)\n","df_expeditions = df_expeditions.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_expeditions.head(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:39.355862Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:41.2736045Z","execution_finish_time":"2025-12-16T14:43:41.5463495Z","parent_msg_id":"1982b210-fc12-4f6e-81ce-c2a1a28b8ad4"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 29, Finished, Available, Finished)"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"1bc20969-27aa-43ad-b927-8026a28f9b24"},{"cell_type":"markdown","source":["And now I'll filter the Members dataframe using the Peaks dataframe. So again:\n","1. Rename the PEAKID column in the Members dataframe\n","2. Join the Members with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e52dc4a5-0294-4d30-868f-0211d75ced15"},{"cell_type":"code","source":["df_members = df_members.withColumnRenamed('PEAKID', 'PEAKID_MEM')\n","df_members = df_members.join(df_peaks, df_members.PEAKID_MEM == df_peaks.PEAKID)\n","df_members = df_members.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_members.head(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":30,"statement_ids":[30],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:39.5280299Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:41.548177Z","execution_finish_time":"2025-12-16T14:43:41.8173551Z","parent_msg_id":"be02924f-7bc9-4e44-a556-699876114d1d"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 30, Finished, Available, Finished)"},"metadata":{}}],"execution_count":28,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"5cc77184-a962-4f78-b389-a7ca600ccbf6"},{"cell_type":"markdown","source":["After filtering these files, I'm changing the types and names for each column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1d18ad9-0f4b-4e20-b696-b7d6f3eed804"},{"cell_type":"code","source":["df_peaks = (\n","    df_peaks\n","    .select(\n","        F.col('PEAKID').alias('PeakID'),\n","        F.col('PKNAME').alias('PeakName'),\n","        F.col('PKNAME2').alias('AlternativePeakName'),\n","        F.col('LOCATION').alias('Location'),\n","        F.col('HEIGHTM').cast(T.IntegerType()).alias('Height_m'),\n","        F.col('OPEN').alias('Open'),\n","        F.col('PEXPID').alias('ExpeditionID'),\n","        F.date_format(F.to_date(F.concat_ws(' ', df_peaks.PYEAR, df_peaks.PSMTDATE), 'yyyy MMM dd'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.col('PSUMMITERS').alias('Summiters'),\n","        F.col('PCOUNTRY').alias('SummitersCountry')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":31,"statement_ids":[31],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:39.7576516Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:41.8192121Z","execution_finish_time":"2025-12-16T14:43:42.0647057Z","parent_msg_id":"c6e1d885-27dc-4015-9a35-42d603f0fa7e"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 31, Finished, Available, Finished)"},"metadata":{}}],"execution_count":29,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52477fce-087e-4fab-8a1f-bda1a67f047c"},{"cell_type":"code","source":["df_expeditions = (\n","    df_expeditions\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_EXP').alias('PeakID'),\n","        F.col('YEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('ROUTE1').alias('Route'),\n","        F.col('NATION').alias('CountryOfOrigin'),\n","        F.col('LEADERS').alias('ExpeditionLeaders'),\n","        F.col('SPONSOR').alias('Sponsor'),\n","        F.col('SUCCESS1').alias('SummitSuccess'),\n","        F.date_format(F.to_date(F.col('BCDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('BaseCampDate'),\n","        F.date_format(F.to_date(F.col('SMTDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('SMTTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('SMTTIME'), 3, 2)).alias('SummitTime'),\n","        F.col('SMTDAYS').cast(T.ShortType()).alias('DaysToSummit'),\n","        F.col('TERMDATE').alias('ExpeditionEndDate'),\n","        F.col('TERMNOTE').alias('Notes'),\n","        F.col('CAMPS').cast(T.ByteType()).alias('Camps'),\n","        F.col('Rope').alias('RopeUsed_m'),\n","        F.col('TOTMEMBERS').cast(T.ByteType()).alias('TotalMembers'),\n","        F.col('SMTMEMBERS').cast(T.ByteType()).alias('SummitMembers'),\n","        F.col('MDEATHS').cast(T.ByteType()).alias('MembersDeath'),\n","        F.col('TOTHIRED').cast(T.ByteType()).alias('TotalHired'),\n","        F.col('SMTHIRED').cast(T.ByteType()).alias('SummitHired'),\n","        F.col('HDEATHS').cast(T.ByteType()).alias('HiredDeath'),\n","        F.col('O2USED').alias('UsedO2'),\n","        F.col('OTHERSMTS').alias('OtherSummits'),\n","        F.col('CAMPSITES').alias('Campsites'),\n","        F.col('ACCIDENTS').alias('Accidents'),\n","        F.col('ACHIEVMENT').alias('Achievement')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:39.95583Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:42.0668461Z","execution_finish_time":"2025-12-16T14:43:42.9166127Z","parent_msg_id":"128ac040-77da-49fa-a95c-43a95a0c2e11"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 32, Finished, Available, Finished)"},"metadata":{}}],"execution_count":30,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c052e34e-aa8d-4c66-9172-bcfa2a15234e"},{"cell_type":"code","source":["df_members = (\n","    df_members\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_MEM').alias('PeakID'),\n","        F.col('MYEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('FNAME').alias('FirstName'),\n","        F.col('LNAME').alias('LastName'),\n","        F.col('Sex').alias('Gender'),\n","        F.col('YOB').cast(T.ShortType()).alias('YearOfBirth'),\n","        F.col('CALCAGE').cast(T.ByteType()).alias('Age'),\n","        F.col('CITIZEN').alias('Nationality'),\n","        F.col('STATUS').alias('Status'),\n","        F.col('OCCUPATION').alias('Occupation'),\n","        F.col('LEADER').alias('Leader'),\n","        F.col('SUPPORT').alias('Support'),\n","        F.col('DISABLED').alias('Disabled'),\n","        F.col('SHERPA').alias('Sherpa'),\n","        F.col('TIBETAN').alias('Tibetan'),\n","        F.col('MSUCCESS').alias('SummitSuccess'),\n","        F.col('MSOLO').alias('SoloSummit'),\n","        F.date_format(F.to_date(F.col('MSMTDATE1'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('MSMTTIME1'), 1, 2), F.lit(\":\"), F.substring(F.col('MSMTTIME1'), 3, 2)).alias('SummitTime'),\n","        F.col('MROUTE1').alias('MountainRoute'),\n","        F.col('MASCENT1').alias('AscentRoute'),\n","        F.col('MO2USED').alias('UsedO2'),\n","        F.col('DEATH').alias('Death'),\n","        F.date_format(F.to_date(F.col('DEATHDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('DeathDate'),\n","        F.concat(F.substring(F.col('DEATHTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('DEATHTIME'), 3, 2)).alias('DeathTime'),\n","        F.col('DEATHNOTE').alias('DeathNote'),\n","        F.col('INJURY').alias('Injury'),\n","        F.date_format(F.to_date(F.col('INJURYDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('InjuryDate'),\n","        F.concat(F.substring(F.col('INJURYTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('INJURYTIME'), 3, 2)).alias('InjuryTime')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:40.1688022Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:42.9188771Z","execution_finish_time":"2025-12-16T14:43:43.7902659Z","parent_msg_id":"75e9283f-9937-418b-9b36-9656aa8289fa"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 33, Finished, Available, Finished)"},"metadata":{}}],"execution_count":31,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f917f830-9660-4f29-b23d-daec9b9f9255"},{"cell_type":"markdown","source":["### Weather data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d3890cd-663c-4ee5-86f0-67b83a25e81b"},{"cell_type":"markdown","source":["Next up I'm going to treat the weather data, starting with the current weather data.\n","\n","The values here are stored in a \"key: value\" format, so I need to extract each value in order to create a table with the columns I'm interested in.\n","\n","It's important to note that Datetime is reflecting the timezone in Mount Everest, which is GMT + 8h."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd76d164-d408-4820-8065-40dc113dfbb5"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_weather_data.json\")\n","df_current_weather = (df.select(\n","    F.date_format(F.col('current.time'), 'yyyy-MM-dd').alias('Date'),\n","    F.date_format(F.col('current.time'), 'HH:mm').alias('Time'),\n","    F.date_format(F.col('daily.sunrise').getItem(0), 'HH:mm').alias('Sunrise'), \n","    F.date_format(F.col('daily.sunset').getItem(0), 'HH:mm').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('current.weather_code').alias('WeatherCode'),\n","    F.col('current.precipitation').alias('Precipitation_mm'),\n","    F.col('current.temperature_2m').alias('Temperature_ºC'),\n","    F.col('current.wind_speed_10m').alias('WindSpeed_km/h'),\n","    F.col('current.wind_gusts_10m').alias('WindGusts_km/h'),\n","    F.col('current.relative_humidity_2m').alias('RelativeHumidity_%'),\n","    F.col('current.snowfall').alias('Snowfall_cm')\n","))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:40.4164476Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:43.7924603Z","execution_finish_time":"2025-12-16T14:43:44.6520063Z","parent_msg_id":"03c88b13-b66d-4a5b-8229-09a2261df5af"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 34, Finished, Available, Finished)"},"metadata":{}}],"execution_count":32,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false},"editable":true,"run_control":{"frozen":false}},"id":"df933deb-4a48-40f2-81ff-c8dddf40d8b8"},{"cell_type":"markdown","source":["Next I'm organizing the information for historical weather data. I need to expand the values in different columns because each column had the results stored for all dates and I need to convert that to multiple rows."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"063ca284-332a-4be1-a928-3b66a73cfa62"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_hist_weather_data.json\")\n","df = df.select(\n","    F.col('hourly.time').alias('Datetime'),\n","    F.col('daily.time').alias('Date'),\n","    F.col('daily.sunrise').alias('Sunrise'),\n","    F.col('daily.sunset').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('hourly.weather_code').alias('WeatherCode'),\n","    F.col('hourly.precipitation').alias('Precipitation'), \n","    F.col('hourly.temperature_2m').alias('Temperature'),\n","    F.col('hourly.wind_speed_10m').alias('WindSpeed'),\n","    F.col('hourly.wind_gusts_10m').alias('WindGusts'),\n","    F.col('hourly.relative_humidity_2m').alias('RelativeHumidity'),\n","    F.col('hourly.snowfall').alias('Snowfall')\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:40.7431802Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:44.6538581Z","execution_finish_time":"2025-12-16T14:43:48.3205023Z","parent_msg_id":"8f08999d-7541-43cf-aeeb-21bd2abfc323"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 35, Finished, Available, Finished)"},"metadata":{}}],"execution_count":33,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false},"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8d3f6414-c2a9-4e2c-afdf-ed8673be9a5c"},{"cell_type":"code","source":["df_hist_weather = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Datetime'),\n","            F.col('WeatherCode'),\n","            F.col('Precipitation'),\n","            F.col('Temperature'),\n","            F.col('WindSpeed'),\n","            F.col('WindGusts'),\n","            F.col('RelativeHumidity'),\n","            F.col('Snowfall')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.date_format(F.col('zipped.Datetime'), 'yyyy-MM-dd').alias('Date'),\n","        F.date_format(F.col('zipped.Datetime'), 'HH:mm').alias('Time'),\n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone'),\n","        F.col('zipped.WeatherCode').alias('WeatherCode'),\n","        F.col('zipped.Precipitation').alias('Precipitation_mm'),\n","        F.col('zipped.Temperature').alias('Temperature_ºC'),\n","        F.col('zipped.WindSpeed').alias('WindSpeed_km/h'),\n","        F.col('zipped.WindGusts').alias('WindGusts_km/h'),\n","        F.col('zipped.RelativeHumidity').alias('RelativeHumidity_%'),\n","        F.col('zipped.Snowfall').alias('Snowfall_cm')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:40.9277689Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:48.3228477Z","execution_finish_time":"2025-12-16T14:43:49.1514597Z","parent_msg_id":"3949dce0-d121-4fd7-86ad-a8f190262202"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 36, Finished, Available, Finished)"},"metadata":{}}],"execution_count":34,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386758a6-2be8-4964-ac7a-42a51c45c2de"},{"cell_type":"markdown","source":["I'm creating a separate dataframe to store the sunrise and sunset values."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3738379b-71ed-4621-84ac-3c5629357d82"},{"cell_type":"code","source":["df_weather_daily = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Date'),\n","            F.col('Sunrise'),\n","            F.col('Sunset')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.col('zipped.Date').alias('Date'),\n","        F.date_format(F.col('zipped.Sunrise'), 'HH:mm').alias('Sunrise'),   \n","        F.date_format(F.col('zipped.Sunset'), 'HH:mm').alias('Sunset'),     \n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone')\n","    )\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:41.1234981Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:49.1533993Z","execution_finish_time":"2025-12-16T14:43:49.4357268Z","parent_msg_id":"14f7532e-87db-4568-a6d9-f5f669066257"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 37, Finished, Available, Finished)"},"metadata":{}}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"541f6703-0e24-4d39-802f-45c7a6a720c6"},{"cell_type":"markdown","source":["I now need to import the information about the weather codes that is also stored in a csv file and that was previously imported directly to the lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6c625eb-b84e-4dac-9377-aa4e40b7b38b"},{"cell_type":"code","source":["df_weather_codes = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/weather_codes.csv\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"finished","queued_time":"2025-12-16T14:43:41.6733468Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:49.4376006Z","execution_finish_time":"2025-12-16T14:43:50.2762707Z","parent_msg_id":"f4e288d8-5ac8-48e0-96b7-9c1d2f70d11a"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 38, Finished, Available, Finished)"},"metadata":{}}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b4c635d-cd8b-4fdc-98a1-f7b93ddf9cb0"},{"cell_type":"markdown","source":["Finally, I'm saving this information as a table for the gold layer."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"029109dc-7528-4851-a1b9-38afffc5e3b1"},{"cell_type":"code","source":["df_expeditions.write.mode('overwrite').saveAsTable('expeditions_silver')\n","df_peaks.write.mode('overwrite').saveAsTable('peaks_silver')\n","df_members.write.mode('overwrite').saveAsTable('members_silver')\n","df_current_weather.write.mode('overwrite').saveAsTable('weather_current_silver')\n","df_hist_weather.write.mode('overwrite').saveAsTable('weather_hist_silver')\n","df_weather_codes.write.mode('overwrite').saveAsTable('weather_codes_silver')\n","df_weather_daily.write.mode('overwrite').saveAsTable('weather_daily_silver')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":39,"statement_ids":[39],"state":"submitted","livy_statement_state":"running","session_id":"54185637-6133-459d-a9ba-b893679bc3e9","normalized_state":"running","queued_time":"2025-12-16T14:43:42.5993438Z","session_start_time":null,"execution_start_time":"2025-12-16T14:43:50.2781844Z","execution_finish_time":null,"parent_msg_id":"4d50a6a6-3d15-47bb-afa4-6b2ade856c0a"},"text/plain":"StatementMeta(, 54185637-6133-459d-a9ba-b893679bc3e9, 39, Submitted, Running, Running)"},"metadata":{}}],"execution_count":37,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"c4fe32fc-16e6-40ef-93b5-9c090eaddef1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"e8d581c4-dedd-451f-841b-d7aad8f1a690","known_lakehouses":[{"id":"e8d581c4-dedd-451f-841b-d7aad8f1a690"}],"default_lakehouse_name":"EverestWeather","default_lakehouse_workspace_id":"fc9513c7-33cd-4dd8-9e35-9951846a8312"}}},"nbformat":4,"nbformat_minor":5}