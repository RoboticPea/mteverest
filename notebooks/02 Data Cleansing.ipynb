{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":{"default_lakehouse":"e8d581c4-dedd-451f-841b-d7aad8f1a690","default_lakehouse_name":"EverestWeather","default_lakehouse_workspace_id":"fc9513c7-33cd-4dd8-9e35-9951846a8312","known_lakehouses":[{"id":"e8d581c4-dedd-451f-841b-d7aad8f1a690"}]}},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"54d69e8e-ab3f-47ef-961a-68a8777b4b71":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":{"snowfall":0,"relative_humidity_2m":17,"wind_speed_10m":10.4,"interval":900,"temperature_2m":-28.6,"wind_gusts_10m":24.1,"time":"2025-12-05T23:15","precipitation":0},"1":{"snowfall":"cm","relative_humidity_2m":"%","wind_speed_10m":"km/h","interval":"seconds","temperature_2m":"°C","wind_gusts_10m":"km/h","time":"iso8601","precipitation":"mm"},"2":{"sunrise":["2025-12-05T08:49"],"sunset":["2025-12-05T19:16"],"time":["2025-12-05"]},"3":{"sunrise":"iso8601","sunset":"iso8601","time":"iso8601"},"4":"8724.0","5":"0.11360645294189453","6":"28.0","7":"86.875","8":"Asia/Shanghai","9":"GMT+8","10":"28800"}],"schema":[{"key":"0","name":"current","type":"StructType(StructField(interval,LongType,true),StructField(precipitation,DoubleType,true),StructField(relative_humidity_2m,LongType,true),StructField(snowfall,DoubleType,true),StructField(temperature_2m,DoubleType,true),StructField(time,StringType,true),StructField(wind_gusts_10m,DoubleType,true),StructField(wind_speed_10m,DoubleType,true))"},{"key":"1","name":"current_units","type":"StructType(StructField(interval,StringType,true),StructField(precipitation,StringType,true),StructField(relative_humidity_2m,StringType,true),StructField(snowfall,StringType,true),StructField(temperature_2m,StringType,true),StructField(time,StringType,true),StructField(wind_gusts_10m,StringType,true),StructField(wind_speed_10m,StringType,true))"},{"key":"2","name":"daily","type":"StructType(StructField(sunrise,ArrayType(StringType,true),true),StructField(sunset,ArrayType(StringType,true),true),StructField(time,ArrayType(StringType,true),true))"},{"key":"3","name":"daily_units","type":"StructType(StructField(sunrise,StringType,true),StructField(sunset,StringType,true),StructField(time,StringType,true))"},{"key":"4","name":"elevation","type":"double"},{"key":"5","name":"generationtime_ms","type":"double"},{"key":"6","name":"latitude","type":"double"},{"key":"7","name":"longitude","type":"double"},{"key":"8","name":"timezone","type":"string"},{"key":"9","name":"timezone_abbreviation","type":"string"},{"key":"10","name":"utc_offset_seconds","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}},"a9323c10-4c4d-4da6-bdaf-46489920befd":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2025-12-05T23:15"}],"schema":[{"key":"0","name":"time","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}},"557e3f21-7860-45f1-b551-e75b75ade134":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2025-12-05T23:15","1":"2025-12-05T08:49","2":"2025-12-05T19:16","3":"0.0","4":"mm","5":"-28.6","6":"°C","7":"10.4","8":"km/h","9":"24.1","10":"km/h","11":"17","12":"%","13":"0.0","14":"cm"}],"schema":[{"key":"0","name":"Time","type":"string"},{"key":"1","name":"Sunrise","type":"string"},{"key":"2","name":"Sunset","type":"string"},{"key":"3","name":"Precipitation","type":"double"},{"key":"4","name":"Precipitation_unit","type":"string"},{"key":"5","name":"Temperature","type":"double"},{"key":"6","name":"Temperature_unit","type":"string"},{"key":"7","name":"Wind_speed","type":"double"},{"key":"8","name":"Wind_speed_unit","type":"string"},{"key":"9","name":"Wind_gusts","type":"double"},{"key":"10","name":"Wind_gusts_unit","type":"string"},{"key":"11","name":"Relative_humidity","type":"bigint"},{"key":"12","name":"Relative_humidity_unit","type":"string"},{"key":"13","name":"Snowfall","type":"double"},{"key":"14","name":"Snowfall_unit","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df_current_weather"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"cells":[{"id":"ea68e5cd-d5cf-49eb-96cd-4d0b5a84dc5d","cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Mt. Everest Data Cleansing \n","Silver Layer Processing"]},{"id":"c7c7d8a5-c2df-49e7-b0e8-2b9b6c26c0e7","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["from pyspark.sql import functions as F, types as T, DataFrame as Frame, Column as C "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":73,"statement_ids":[73],"state":"finished","livy_statement_state":"available","session_id":"4bc1890a-70c6-4f59-8462-f3eaeee41cea","normalized_state":"finished","queued_time":"2025-12-05T14:59:56.4212707Z","session_start_time":null,"execution_start_time":"2025-12-05T14:59:56.4224059Z","execution_finish_time":"2025-12-05T14:59:56.7841296Z","parent_msg_id":"13c16c74-09ef-447a-9501-d369b4b9aeee"},"text/plain":"StatementMeta(, 4bc1890a-70c6-4f59-8462-f3eaeee41cea, 73, Finished, Available, Finished)"},"metadata":{}}],"execution_count":71},{"id":"b30024aa-e031-4a9c-afa4-08eb2642a072","cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["Starting with the Expedition data, I'll take that data from the files and create a dataframe for each file."]},{"id":"f6a7042c-6679-4ec2-b0c6-c9e7dea601a1","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["df_expeditions = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/expeditions.csv\")\n","df_peaks = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/peaks.csv\")\n","df_members = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/members.csv\")\n","df_refer = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/refer.csv\")"],"outputs":[]},{"id":"81b2ccf5-4d4e-465e-b450-f43f3c8a35e5","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["I just want to import data about Everest, so I'll start with removing all other rows from Peaks."]},{"id":"072d0e0a-f358-4848-95ee-7ca215a8ba4a","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["df_peaks = df_peaks.filter(df_peaks.PKNAME == 'Everest')\n","display(df_peaks)"],"outputs":[]},{"id":"7f3c4450-8b7b-4343-bbb4-b392994e18c4","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["Next I'm doing the same concerning the Expeditions, but for this I need to do three steps:\n","1. Rename the PEAKID column in the Expeditions dataframe\n","2. Join the Expeditions with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe \n","\n","I started by renaming the column PEAKID from the Expeditions to PEAKID_EXP so that when dropping the extra columns there would not be a duplicate PEAKID column."]},{"id":"1bc20969-27aa-43ad-b927-8026a28f9b24","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["df_expeditions = df_expeditions.withColumnRenamed('PEAKID', 'PEAKID_EXP')\n","df_expeditions = df_expeditions.join(df_peaks, df_expeditions.PEAKID_EXP == df_peaks.PEAKID)\n","df_expeditions = df_expeditions.drop(*(F.col(c) for c in df_peaks.columns))\n","display(df_expeditions.head(10))"],"outputs":[]},{"id":"28f82e04-f684-459d-8eef-df7e436737e7","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["Similarly, I'm now going to do the same three steps for the Refer dataframe:\n","1. Rename the EXPID column in the Refer dataframe\n","2. Join the Refer with the Expeditions using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Expeditions dataframe "]},{"id":"77f965e7-abcf-4e56-8e01-38c754322c76","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["df_refer = df_refer.withColumnRenamed('EXPID', 'EXPID_REF')\n","df_refer = df_refer.join(df_expeditions, df_refer.EXPID_REF == df_expeditions.EXPID)\n","df_refer = df_refer.drop(*(F.col(c) for c in df_expeditions.columns))\n","display(df_refer.head(10))"],"outputs":[]},{"id":"e52dc4a5-0294-4d30-868f-0211d75ced15","cell_type":"markdown","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["And finally I'll filter the Members dataframe as well using the Peaks dataframe. So again:\n","1. Rename the PEAKID column in the Members dataframe\n","2. Join the Members with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe "]},{"id":"5cc77184-a962-4f78-b389-a7ca600ccbf6","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["df_members = df_members.withColumnRenamed('PEAKID', 'PEAKID_MEM')\n","df_members = df_members.join(df_peaks, df_members.PEAKID_MEM == df_peaks.PEAKID)\n","df_members = df_members.drop(*(F.col(c) for c in df_peaks.columns))\n","display(df_members.head(10))"],"outputs":[]},{"id":"d74abe8d-6770-4a3f-8eca-1b17895f8594","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_weather_data.json\")\n","# df now is a Spark DataFrame containing JSON data from \"Files/mteverest_weather_data.json\".\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":77,"statement_ids":[77],"state":"finished","livy_statement_state":"available","session_id":"4bc1890a-70c6-4f59-8462-f3eaeee41cea","normalized_state":"finished","queued_time":"2025-12-05T15:15:20.0080037Z","session_start_time":null,"execution_start_time":"2025-12-05T15:15:20.0091703Z","execution_finish_time":"2025-12-05T15:15:21.5761594Z","parent_msg_id":"d172b2a0-ad9e-4b9d-8c99-9e0bc12bd688"},"text/plain":"StatementMeta(, 4bc1890a-70c6-4f59-8462-f3eaeee41cea, 77, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"54d69e8e-ab3f-47ef-961a-68a8777b4b71","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 54d69e8e-ab3f-47ef-961a-68a8777b4b71)"},"metadata":{}}],"execution_count":75},{"id":"a099eb9c-b6dc-4fad-a25b-bba1d060870a","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["display(df.select('current.time'))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":80,"statement_ids":[80],"state":"finished","livy_statement_state":"available","session_id":"4bc1890a-70c6-4f59-8462-f3eaeee41cea","normalized_state":"finished","queued_time":"2025-12-05T15:20:58.8624357Z","session_start_time":null,"execution_start_time":"2025-12-05T15:20:58.8636044Z","execution_finish_time":"2025-12-05T15:20:59.6879431Z","parent_msg_id":"4f482270-20a6-48e8-9bf9-cd8162c67f99"},"text/plain":"StatementMeta(, 4bc1890a-70c6-4f59-8462-f3eaeee41cea, 80, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"a9323c10-4c4d-4da6-bdaf-46489920befd","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, a9323c10-4c4d-4da6-bdaf-46489920befd)"},"metadata":{}}],"execution_count":78},{"id":"b79799b3-9957-4e53-b94a-7f8bb3fe5134","cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"source":["df_current_weather = (df.select(\n","    F.col('current.time').alias('Time'),\n","    F.col('daily.sunrise').getItem(0).alias('Sunrise'),\n","    F.col('daily.sunset').getItem(0).alias('Sunset'),\n","    F.col('current.precipitation').alias('Precipitation'),\n","    F.col('current_units.precipitation').alias('Precipitation_unit'),  \n","    F.col('current.temperature_2m').alias('Temperature'),\n","    F.col('current_units.temperature_2m').alias('Temperature_unit'),\n","    F.col('current.wind_speed_10m').alias('Wind_speed'),\n","    F.col('current_units.wind_speed_10m').alias('Wind_speed_unit'),\n","    F.col('current.wind_gusts_10m').alias('Wind_gusts'),\n","    F.col('current_units.wind_gusts_10m').alias('Wind_gusts_unit'),\n","    F.col('current.relative_humidity_2m').alias('Relative_humidity'),\n","    F.col('current_units.relative_humidity_2m').alias('Relative_humidity_unit'),\n","    F.col('current.snowfall').alias('Snowfall'),\n","    F.col('current_units.snowfall').alias('Snowfall_unit')\n","))\n","display(df_current_weather)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":86,"statement_ids":[86],"state":"finished","livy_statement_state":"available","session_id":"4bc1890a-70c6-4f59-8462-f3eaeee41cea","normalized_state":"finished","queued_time":"2025-12-05T15:25:27.4485986Z","session_start_time":null,"execution_start_time":"2025-12-05T15:25:27.4497214Z","execution_finish_time":"2025-12-05T15:25:28.2973639Z","parent_msg_id":"1364031f-7831-4f75-af4c-45104c15863d"},"text/plain":"StatementMeta(, 4bc1890a-70c6-4f59-8462-f3eaeee41cea, 86, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"557e3f21-7860-45f1-b551-e75b75ade134","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 557e3f21-7860-45f1-b551-e75b75ade134)"},"metadata":{}}],"execution_count":84},{"id":"1fe3087f-5a58-4c0c-b86b-e75a2772b567","cell_type":"code","metadata":{"run_control":{"frozen":true},"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_hist_weather_data.json\")\n","# df now is a Spark DataFrame containing JSON data from \"Files/mteverest_hist_weather_data.json\".\n","df.show()"],"outputs":[]}]}