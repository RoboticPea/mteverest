{"cells":[{"cell_type":"markdown","source":["# Mt. Everest Data Cleansing \n","Silver Layer Processing"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ea68e5cd-d5cf-49eb-96cd-4d0b5a84dc5d"},{"cell_type":"markdown","source":["### Importing libs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7957a7fa-3c55-428b-9ce6-23e40b0f95f1"},{"cell_type":"code","source":["from pyspark.sql import functions as F, types as T, DataFrame as Fr, Column as C "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7c7d8a5-c2df-49e7-b0e8-2b9b6c26c0e7"},{"cell_type":"markdown","source":["### Himalayan Database data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1bf19fe-5da7-45d9-8590-e83bbe639586"},{"cell_type":"markdown","source":["Starting with the Expedition data, I'm going to create a dataframe for each file to start working with it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b30024aa-e031-4a9c-afa4-08eb2642a072"},{"cell_type":"code","source":["df_expeditions = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/expeditions.csv\")\n","df_peaks = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/peaks.csv\")\n","df_members = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/members.csv\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"f6a7042c-6679-4ec2-b0c6-c9e7dea601a1"},{"cell_type":"markdown","source":["I just want to import data about Everest, so I'll start with removing all other rows from Peaks - this with be the starting point for the upcoming filters."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81b2ccf5-4d4e-465e-b450-f43f3c8a35e5"},{"cell_type":"code","source":["df_peaks = df_peaks.filter(df_peaks.PKNAME == 'Everest')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"072d0e0a-f358-4848-95ee-7ca215a8ba4a"},{"cell_type":"markdown","source":["Next I'm doing the same concerning the Expeditions, but for this I need to do three steps:\n","1. Rename the PEAKID column in the Expeditions dataframe\n","2. Join the Expeditions with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe \n","\n","I started by renaming the column PEAKID from the Expeditions to PEAKID_EXP so that when dropping the extra columns there would not be a duplicate PEAKID column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f3c4450-8b7b-4343-bbb4-b392994e18c4"},{"cell_type":"code","source":["df_expeditions = df_expeditions.withColumnRenamed('PEAKID', 'PEAKID_EXP')\n","df_expeditions = df_expeditions.join(df_peaks, df_expeditions.PEAKID_EXP == df_peaks.PEAKID)\n","df_expeditions = df_expeditions.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_expeditions.head(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"1bc20969-27aa-43ad-b927-8026a28f9b24"},{"cell_type":"markdown","source":["And now I'll filter the Members dataframe using the Peaks dataframe. So again:\n","1. Rename the PEAKID column in the Members dataframe\n","2. Join the Members with the Peaks using the PEAKID column\n","3. Clearing from the resulting dataframe all the columns that were imported from the Peaks dataframe "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e52dc4a5-0294-4d30-868f-0211d75ced15"},{"cell_type":"code","source":["df_members = df_members.withColumnRenamed('PEAKID', 'PEAKID_MEM')\n","df_members = df_members.join(df_peaks, df_members.PEAKID_MEM == df_peaks.PEAKID)\n","df_members = df_members.drop(*(F.col(c) for c in df_peaks.columns))\n","#display(df_members.head(10))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false}},"id":"5cc77184-a962-4f78-b389-a7ca600ccbf6"},{"cell_type":"markdown","source":["After filtering these files, I'm changing the types and names for each column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1d18ad9-0f4b-4e20-b696-b7d6f3eed804"},{"cell_type":"code","source":["df_peaks = (\n","    df_peaks\n","    .select(\n","        F.col('PEAKID').alias('PeakID'),\n","        F.col('PKNAME').alias('PeakName'),\n","        F.col('PKNAME2').alias('AlternativePeakName'),\n","        F.col('LOCATION').alias('Location'),\n","        F.col('HEIGHTM').cast(T.IntegerType()).alias('Height_m'),\n","        F.col('OPEN').alias('Open'),\n","        F.col('PEXPID').alias('ExpeditionID'),\n","        F.date_format(F.to_date(F.concat_ws(' ', df_peaks.PYEAR, df_peaks.PSMTDATE), 'yyyy MMM dd'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.col('PSUMMITERS').alias('Summiters'),\n","        F.col('PCOUNTRY').alias('SummitersCountry')\n","    )\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52477fce-087e-4fab-8a1f-bda1a67f047c"},{"cell_type":"code","source":["df_expeditions = (\n","    df_expeditions\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_EXP').alias('PeakID'),\n","        F.col('YEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('ROUTE1').alias('Route'),\n","        F.col('NATION').alias('CountryOfOrigin'),\n","        F.col('LEADERS').alias('ExpeditionLeaders'),\n","        F.col('SPONSOR').alias('Sponsor'),\n","        F.col('SUCCESS1').alias('SummitSuccess'),\n","        F.date_format(F.to_date(F.col('BCDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('BaseCampDate'),\n","        F.date_format(F.to_date(F.col('SMTDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('SMTTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('SMTTIME'), 3, 2)).alias('SummitTime'),\n","        F.col('SMTDAYS').cast(T.ShortType()).alias('DaysToSummit'),\n","        F.date_format(F.to_date(F.col('TERMDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('ExpeditionEndDate'),\n","        F.col('TERMNOTE').alias('Notes'),\n","        F.col('CAMPS').cast(T.ByteType()).alias('Camps'),\n","        F.col('Rope').alias('RopeUsed_m'),\n","        F.col('TOTMEMBERS').cast(T.ByteType()).alias('TotalMembers'),\n","        F.col('SMTMEMBERS').cast(T.ByteType()).alias('SummitMembers'),\n","        F.col('MDEATHS').cast(T.ByteType()).alias('MembersDeath'),\n","        F.col('TOTHIRED').cast(T.ByteType()).alias('TotalHired'),\n","        F.col('SMTHIRED').cast(T.ByteType()).alias('SummitHired'),\n","        F.col('HDEATHS').cast(T.ByteType()).alias('HiredDeath'),\n","        F.col('O2USED').alias('UsedO2'),\n","        F.col('OTHERSMTS').alias('OtherSummits'),\n","        F.col('CAMPSITES').alias('Campsites'),\n","        F.col('ACCIDENTS').alias('Accidents'),\n","        F.col('ACHIEVMENT').alias('Achievement')\n","    )\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c052e34e-aa8d-4c66-9172-bcfa2a15234e"},{"cell_type":"code","source":["df_members = (\n","    df_members\n","    .select(\n","        F.col('EXPID').alias('ExpeditionID'),\n","        F.col('PEAKID_MEM').alias('PeakID'),\n","        F.col('MYEAR').cast(T.ShortType()).alias('ExpeditionYear'),\n","        F.col('FNAME').alias('FirstName'),\n","        F.col('LNAME').alias('LastName'),\n","        F.col('Sex').alias('Gender'),\n","        F.col('YOB').cast(T.ShortType()).alias('YearOfBirth'),\n","        F.col('CALCAGE').cast(T.ByteType()).alias('Age'),\n","        F.col('CITIZEN').alias('Nationality'),\n","        F.col('STATUS').alias('Status'),\n","        F.col('OCCUPATION').alias('Occupation'),\n","        F.col('LEADER').alias('Leader'),\n","        F.col('SUPPORT').alias('Support'),\n","        F.col('DISABLED').alias('Disabled'),\n","        F.col('SHERPA').alias('Sherpa'),\n","        F.col('TIBETAN').alias('Tibetan'),\n","        F.col('MSUCCESS').alias('SummitSuccess'),\n","        F.col('MSOLO').alias('SoloSummit'),\n","        F.date_format(F.to_date(F.col('MSMTDATE1'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('SummitDate'),\n","        F.concat(F.substring(F.col('MSMTTIME1'), 1, 2), F.lit(\":\"), F.substring(F.col('MSMTTIME1'), 3, 2)).alias('SummitTime'),\n","        F.col('MROUTE1').alias('MountainRoute'),\n","        F.col('MASCENT1').alias('AscentRoute'),\n","        F.col('MO2USED').alias('UsedO2'),\n","        F.col('DEATH').alias('Death'),\n","        F.date_format(F.to_date(F.col('DEATHDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('DeathDate'),\n","        F.concat(F.substring(F.col('DEATHTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('DEATHTIME'), 3, 2)).alias('DeathTime'),\n","        F.col('DEATHNOTE').alias('DeathNote'),\n","        F.col('INJURY').alias('Injury'),\n","        F.date_format(F.to_date(F.col('INJURYDATE'), 'dd/MM/yyyy'), 'yyyy-MM-dd').alias('InjuryDate'),\n","        F.concat(F.substring(F.col('INJURYTIME'), 1, 2), F.lit(\":\"), F.substring(F.col('INJURYTIME'), 3, 2)).alias('InjuryTime')\n","    )\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f917f830-9660-4f29-b23d-daec9b9f9255"},{"cell_type":"markdown","source":["### Weather data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d3890cd-663c-4ee5-86f0-67b83a25e81b"},{"cell_type":"markdown","source":["Next up I'm going to treat the weather data, starting with the current weather data.\n","\n","The values here are stored in a \"key: value\" format, so I need to extract each value in order to create a table with the columns I'm interested in.\n","\n","It's important to note that Datetime is reflecting the timezone in Mount Everest, which is GMT + 8h."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd76d164-d408-4820-8065-40dc113dfbb5"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_weather_data.json\")\n","df_current_weather = (df.select(\n","    F.date_format(F.col('current.time'), 'yyyy-MM-dd').alias('Date'),\n","    F.date_format(F.col('current.time'), 'HH:mm').alias('Time'),\n","    F.date_format(F.col('daily.sunrise').getItem(0), 'HH:mm').alias('Sunrise'), \n","    F.date_format(F.col('daily.sunset').getItem(0), 'HH:mm').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('current.weather_code').alias('WeatherCode'),\n","    F.col('current.precipitation').alias('Precipitation_mm'),\n","    F.col('current.temperature_2m').alias('Temperature_ºC'),\n","    F.col('current.wind_speed_10m').alias('WindSpeed_km/h'),\n","    F.col('current.wind_gusts_10m').alias('WindGusts_km/h'),\n","    F.col('current.relative_humidity_2m').alias('RelativeHumidity_%'),\n","    F.col('current.snowfall').alias('Snowfall_cm')\n","))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"outputs_hidden":false},"editable":true,"run_control":{"frozen":false}},"id":"df933deb-4a48-40f2-81ff-c8dddf40d8b8"},{"cell_type":"markdown","source":["Next I'm organizing the information for historical weather data. I need to expand the values in different columns because each column had the results stored for all dates and I need to convert that to multiple rows."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"063ca284-332a-4be1-a928-3b66a73cfa62"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/mteverest_hist_weather_data.json\")\n","df = df.select(\n","    F.col('hourly.time').alias('Datetime'),\n","    F.col('daily.time').alias('Date'),\n","    F.col('daily.sunrise').alias('Sunrise'),\n","    F.col('daily.sunset').alias('Sunset'),\n","    F.col('latitude').alias('Latitude'),\n","    F.col('longitude').alias('Longitude'),\n","    F.col('elevation').alias('Elevation_m'),\n","    F.col('timezone').alias('Location'),\n","    F.col('timezone_abbreviation').alias('Timezone'),\n","    F.col('hourly.weather_code').alias('WeatherCode'),\n","    F.col('hourly.precipitation').alias('Precipitation'), \n","    F.col('hourly.temperature_2m').alias('Temperature'),\n","    F.col('hourly.wind_speed_10m').alias('WindSpeed'),\n","    F.col('hourly.wind_gusts_10m').alias('WindGusts'),\n","    F.col('hourly.relative_humidity_2m').alias('RelativeHumidity'),\n","    F.col('hourly.snowfall').alias('Snowfall')\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"editable":true,"run_control":{"frozen":false},"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8d3f6414-c2a9-4e2c-afdf-ed8673be9a5c"},{"cell_type":"code","source":["df_hist_weather = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Datetime'),\n","            F.col('WeatherCode'),\n","            F.col('Precipitation'),\n","            F.col('Temperature'),\n","            F.col('WindSpeed'),\n","            F.col('WindGusts'),\n","            F.col('RelativeHumidity'),\n","            F.col('Snowfall')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.date_format(F.col('zipped.Datetime'), 'yyyy-MM-dd').alias('Date'),\n","        F.date_format(F.col('zipped.Datetime'), 'HH:mm').alias('Time'),\n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone'),\n","        F.col('zipped.WeatherCode').alias('WeatherCode'),\n","        F.col('zipped.Precipitation').alias('Precipitation_mm'),\n","        F.col('zipped.Temperature').alias('Temperature_ºC'),\n","        F.col('zipped.WindSpeed').alias('WindSpeed_km/h'),\n","        F.col('zipped.WindGusts').alias('WindGusts_km/h'),\n","        F.col('zipped.RelativeHumidity').alias('RelativeHumidity_%'),\n","        F.col('zipped.Snowfall').alias('Snowfall_cm')\n","    )\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386758a6-2be8-4964-ac7a-42a51c45c2de"},{"cell_type":"markdown","source":["I'm creating a separate dataframe to store the sunrise and sunset values."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3738379b-71ed-4621-84ac-3c5629357d82"},{"cell_type":"code","source":["df_weather_daily = (\n","    df\n","    .withColumn(\n","        'zipped',\n","        F.arrays_zip(\n","            F.col('Date'),\n","            F.col('Sunrise'),\n","            F.col('Sunset')\n","        )\n","    )\n","    .withColumn('zipped', F.explode(F.col('zipped')))\n","    .select(\n","        F.col('zipped.Date').alias('Date'),\n","        F.date_format(F.col('zipped.Sunrise'), 'HH:mm').alias('Sunrise'),   \n","        F.date_format(F.col('zipped.Sunset'), 'HH:mm').alias('Sunset'),     \n","        F.col('Latitude'),\n","        F.col('Longitude'),\n","        F.col('Elevation_m'),\n","        F.col('Location'),\n","        F.col('Timezone')\n","    )\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"541f6703-0e24-4d39-802f-45c7a6a720c6"},{"cell_type":"markdown","source":["I now need to import the information about the weather codes that is also stored in a csv file and that was previously imported directly to the lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6c625eb-b84e-4dac-9377-aa4e40b7b38b"},{"cell_type":"code","source":["df_weather_codes = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\", \";\").load(\"Files/weather_codes.csv\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b4c635d-cd8b-4fdc-98a1-f7b93ddf9cb0"},{"cell_type":"markdown","source":["Finally, I'm saving this information as a table for the gold layer."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"029109dc-7528-4851-a1b9-38afffc5e3b1"},{"cell_type":"code","source":["df_expeditions.write.mode('overwrite').saveAsTable('expeditions_silver')\n","df_peaks.write.mode('overwrite').saveAsTable('peaks_silver')\n","df_members.write.mode('overwrite').saveAsTable('members_silver')\n","df_current_weather.write.mode('overwrite').saveAsTable('weather_current_silver')\n","df_hist_weather.write.mode('overwrite').saveAsTable('weather_hist_silver')\n","df_weather_codes.write.mode('overwrite').saveAsTable('weather_codes_silver')\n","df_weather_daily.write.mode('overwrite').saveAsTable('weather_daily_silver')"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"c4fe32fc-16e6-40ef-93b5-9c090eaddef1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"e8d581c4-dedd-451f-841b-d7aad8f1a690","known_lakehouses":[{"id":"e8d581c4-dedd-451f-841b-d7aad8f1a690"}],"default_lakehouse_name":"EverestWeather","default_lakehouse_workspace_id":"fc9513c7-33cd-4dd8-9e35-9951846a8312"}}},"nbformat":4,"nbformat_minor":5}